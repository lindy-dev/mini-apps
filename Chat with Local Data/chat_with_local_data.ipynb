{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.llms.ollama import Ollama\n",
    "import gradio as gr\n",
    "import time\n",
    "import os.path\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    "    SummaryIndex,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "import shutil\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "from llama_index.core.query_engine import RouterQueryEngine\n",
    "from llama_index.core.selectors import LLMSingleSelector\n",
    "from llama_index.core.node_parser import SentenceSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Llama-index with local Ollama models (llama3 and mxbai-embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The author grew up working on writing and programming. Before college, he wrote short stories and tried to write programs on an IBM 1401 using an early version of Fortran. Later, with the advent of microcomputers, he started programming in earnest and built his own computer, a TRS-80, which was a kit sold by Heathkit.\n"
     ]
    }
   ],
   "source": [
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "\n",
    "# bge-base embedding model\n",
    "Settings.embed_model = OllamaEmbedding(model_name=\"mxbai-embed-large:latest\")\n",
    "\n",
    "# ollama\n",
    "Settings.llm = Ollama(model=\"llama3:latest\", request_timeout=360.0)\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    ")\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What did the author do growing up?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing out ReAct Agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step def9f7a6-839f-40a7-8dc6-9f28a418f708. Step input: What is 2 *617?\n",
      "\u001b[1;3;38;5;200mThought: The current language of the user is: not specified. I need to use a tool to help me answer the question.\n",
      "Action: multiply\n",
      "Action Input: {'a': 2, 'b': 617}\n",
      "\u001b[0m\u001b[1;3;34mObservation: 1234\n",
      "\u001b[0m> Running step fb5abbaf-4c46-42e9-93c1-920029f5ea7e. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer\n",
      "Answer: 1234 is not the result of the multiplication, but rather a separate number. To get the correct answer, we can continue using the multiply tool.\n",
      "\u001b[0m1234 is not the result of the multiplication, but rather a separate number. To get the correct answer, we can continue using the multiply tool.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.tools import FunctionTool, ToolMetadata\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core.agent import ReActAgent\n",
    "\n",
    "# define sample Tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two integers and returns the result integer\"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "multiply_tool = FunctionTool.from_defaults(fn=multiply)\n",
    "\n",
    "# initialize llm\n",
    "llm = Ollama(model=\"llama3:latest\")\n",
    "\n",
    "# initialize ReAct agent\n",
    "agent = ReActAgent.from_tools([multiply_tool], llm=llm, verbose=True)\n",
    "\n",
    "response = agent.chat(\"What is 2 *617?\")\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbot (main functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if storage already exists\n",
    "PERSIST_DIR = \"./storage\"\n",
    "# if not os.path.exists(PERSIST_DIR):\n",
    "#     # load the documents and create the index\n",
    "#     documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "#     index = VectorStoreIndex.from_documents(documents)\n",
    "#     # store it for later\n",
    "#     index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
    "# else:\n",
    "#     # load the existing index\n",
    "#     storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
    "#     index = load_index_from_storage(storage_context)\n",
    "\n",
    "\n",
    "def print_like_dislike(x: gr.LikeData):\n",
    "    print(x.index, x.value, x.liked)\n",
    "\n",
    "\n",
    "def add_message(history, message):\n",
    "    for x in message[\"files\"]:\n",
    "        history.append({\"role\": \"user\", \"content\": {\"path\": x}})\n",
    "        ## Code to load each file into vector index\n",
    "        source = x\n",
    "        filename = os.path.basename(x)\n",
    "        destination = './data/'+filename\n",
    "        try:\n",
    "            # Copy the file from `source` to `destination`\n",
    "            shutil.copy(source, destination)\n",
    "            print(f\"File copied successfully from {source} to {destination}\")\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"The specified source file does not exist: {e}\")\n",
    "        except PermissionError as e:\n",
    "            print(f\"Permission denied while accessing the files or directories: {e}\")\n",
    "        except IsADirectoryError as e:\n",
    "            # This exception occurs if `destination` is a directory instead of a file\n",
    "            print(f\"Destination should be a file, but it's a directory: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while copying the file: {e}\")\n",
    "\n",
    "    if message[\"text\"] is not None:\n",
    "        history.append({\"role\": \"user\", \"content\": message[\"text\"]})\n",
    "    return history, gr.MultimodalTextbox(value=None, interactive=False)\n",
    "\n",
    "\n",
    "def bot(history: list):\n",
    "    # response = \"**That's cool!**\"\n",
    "    documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "    # splitter = SentenceSplitter(chunk_size=1024)\n",
    "    # nodes = splitter.get_nodes_from_documents(documents=documents)\n",
    "    \n",
    "    # bge-base embedding model\n",
    "    Settings.embed_model = OllamaEmbedding(model_name=\"mxbai-embed-large:latest\")\n",
    "\n",
    "    # ollama\n",
    "    Settings.llm = Ollama(model=\"qwen2.5:72b-instruct-q2_K\", request_timeout=360.0)\n",
    "\n",
    "    vector_index = VectorStoreIndex.from_documents(documents=documents)\n",
    "    summary_index = SummaryIndex.from_documents(documents=documents)\n",
    "    summary_query_engine = summary_index.as_query_engine()\n",
    "    vector_query_engine = vector_index.as_query_engine(similarity_top_k=3)\n",
    "    \n",
    "    summary_tool = QueryEngineTool.from_defaults(query_engine=summary_query_engine, \n",
    "                                                 description=\"Useful for summarization questions related to given document\")\n",
    "    vector_tool = QueryEngineTool.from_defaults(query_engine=vector_query_engine,\n",
    "                                                description=(\"Useful for retrieving specific context from the given document\"))\n",
    "    query_engine = RouterQueryEngine(\n",
    "        selector=LLMSingleSelector.from_defaults(),\n",
    "        query_engine_tools=[\n",
    "            vector_tool, \n",
    "            # summary_tool\n",
    "        ], \n",
    "        verbose=True \n",
    "        \n",
    "    )\n",
    "\n",
    "    # query_engine_tools = [QueryEngineTool(query_engine=query_engine, \n",
    "    #                                       metadata=ToolMetadata(\n",
    "    #                                           name=\"Uploaded file\", \n",
    "    #                                           description=\"Uploaded file information\"),\n",
    "    #                                         )\n",
    "        \n",
    "    # ]\n",
    "    \n",
    "    # # initialize llm\n",
    "    # llm = Ollama(model=\"llama3:latest\")\n",
    "\n",
    "    # # initialize ReAct agent\n",
    "    # agent = ReActAgent.from_tools([query_engine_tools], llm=llm, verbose=True)\n",
    "    # # print(history[-1]['content'])\n",
    "\n",
    "    \n",
    "    response = query_engine.query(history[-1]['content'])\n",
    "    # response = agent.chat(history[-1]['content'])\n",
    "    # print(response)\n",
    "    # print(type(response))\n",
    "    # print(response.response)\n",
    "    history.append({\"role\": \"assistant\", \"content\": \"\"})\n",
    "    for character in response.response:\n",
    "        history[-1][\"content\"] += character\n",
    "        time.sleep(0.05)\n",
    "        yield history\n",
    "    # return history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbot UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File copied successfully from /tmp/gradio/d1d658be95b28f084190e9113783c54974efc0fa669e901ad3fd48b68a9c3afc/2309.02427v3.pdf to ./data/2309.02427v3.pdf\n",
      "\u001b[1;3;38;5;200mSelecting query engine 0: The summary suggests that the content of the document can be retrieved, which implies that it contains information about the title as well..\n",
      "\u001b[0m\u001b[1;3;38;5;200mSelecting query engine 0: The question 'What is the Cognitive Architecture for language agents?' requires specific information that can be retrieved from a relevant document, making it the most suitable use case for the provided option..\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(elem_id=\"chatbot\", bubble_full_width=False, type=\"messages\")\n",
    "\n",
    "    chat_input = gr.MultimodalTextbox(\n",
    "        interactive=True,\n",
    "        file_count=\"multiple\",\n",
    "        placeholder=\"Enter message or upload file...\",\n",
    "        show_label=False,\n",
    "    )\n",
    "\n",
    "    chat_msg = chat_input.submit(\n",
    "        add_message, [chatbot, chat_input], [chatbot, chat_input]\n",
    "    )\n",
    "    bot_msg = chat_msg.then(bot, chatbot, chatbot, api_name=\"bot_response\")\n",
    "    bot_msg.then(lambda: gr.MultimodalTextbox(interactive=True), None, [chat_input])\n",
    "\n",
    "    chatbot.like(print_like_dislike, None, None, like_user_message=True)\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Close Demo and delete files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7861\n"
     ]
    }
   ],
   "source": [
    "demo.close()\n",
    "# Specify the directory path\n",
    "directory_path = './data'\n",
    "\n",
    "def delete_files_recursively(directory):\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            # Construct the full file path\n",
    "            file_path = os.path.join(root, file)\n",
    "            try:\n",
    "                # Delete the file\n",
    "                os.remove(file_path)\n",
    "                print(f\"Deleted: {file_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error deleting {file_path}: {e}\")\n",
    "\n",
    "# Call the function to delete files recursively\n",
    "delete_files_recursively(directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chat-locally-rW9mQt3I-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
